{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Network Research - PastNet Hackathon\n",
    "### _Patrick Gildersleve_ - _@pgildersleve_ - _GH:pgilders_ - _patrick.gildersleve [at] oii.ox.ac.uk_\n",
    "\n",
    "The following is a brief tutorial on how to generate and use Wikipedia network data.\n",
    "\n",
    "Useful auxiliary datasets include:\n",
    "- Dumps of lots of different Wikipedia data: https://dumps.wikimedia.org/other/analytics/\n",
    "- API documentation: https://www.mediawiki.org/wiki/API:Properties\n",
    "- Python package for Wikipedia page views: https://github.com/mediawiki-utilities/python-mwviews\n",
    "\n",
    "Contents:\n",
    "1. [Sample Functions](#functions)\n",
    "2. [Generating Networks](#generatingnetworks)\n",
    "    1. [Wikilinks](#wikilinks)\n",
    "    2. [Content-based](#content)\n",
    "    3. [Clickstream](#clickstream)\n",
    "3. [Studying a Network](#studying)\n",
    "4. [Appendix: Running a Query](#query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Functions <a name=\"functions\"></a>\n",
    "Helper Functions (click to expand):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "langlist = ['en', 'fr', 'de'] # edit as appropriate\n",
    "csvpath = '' # edit as appropriate\n",
    "clickstream_dfs = {l:pd.read_csv(csvpath+'clickstream-%swiki-2019-03.tsv' %l,\n",
    "                                   sep = '\\t', index_col=None, header=None, names=['prev', 'curr', 'type', 'n']) for l in langlist}\n",
    "for k, v in clickstream_dfs.items():\n",
    "    clickstream_dfs[k] = v[v['type']=='link'][['prev', 'curr', 'n']]\n",
    "    \n",
    "def query(request, lang):\n",
    "    '''\n",
    "    Make queries to the wikipedia API\n",
    "    '''\n",
    "    request['action'] = 'query'\n",
    "    request['format'] = 'json'\n",
    "    request['redirects'] = ''\n",
    "    lastContinue = {}\n",
    "    while True:\n",
    "        # Clone original request\n",
    "        req = request.copy()\n",
    "        # Modify it with the values returned in the 'continue' section of the last result.\n",
    "        req.update(lastContinue)\n",
    "        # Call API\n",
    "        result = requests.get('https://%s.wikipedia.org/w/api.php' %lang, params=req).json()\n",
    "        if 'error' in result:\n",
    "            print(result['error'])\n",
    "        if 'warnings' in result:\n",
    "            print(result['warnings'])\n",
    "        if 'query' in result:\n",
    "            yield result['query']\n",
    "        if 'continue' not in result:\n",
    "            break\n",
    "        lastContinue = result['continue']\n",
    "\n",
    "\n",
    "def parse(request, lang):\n",
    "    '''\n",
    "    Parse wikipedia pages (or their past revisions)\n",
    "    '''\n",
    "    request['action'] = 'parse'\n",
    "    request['format'] = 'json'\n",
    "    request['redirects'] = ''\n",
    "    lastContinue = {}\n",
    "    while True:\n",
    "        # Clone original request\n",
    "        req = request.copy()\n",
    "        # Modify it with the values returned in the 'continue' section of the last result.\n",
    "        req.update(lastContinue)\n",
    "        # Call API\n",
    "        result = requests.get('https://%s.wikipedia.org/w/api.php' %lang, params=req).json()\n",
    "        if 'error' in result:\n",
    "            print(result['error'])\n",
    "        if 'warnings' in result:\n",
    "            print(result['warnings'])\n",
    "        if 'parse' in result:\n",
    "            yield result['parse']\n",
    "        if 'continue' not in result:\n",
    "            break\n",
    "        lastContinue = result['continue']\n",
    "\n",
    "        \n",
    "def chunks(l, n):\n",
    "    '''\n",
    "    API frequently limits number of pages allowed in one call, this breaks up lists of pages into smaller chunks\n",
    "    '''\n",
    "# For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]\n",
    "        \n",
    "\n",
    "\n",
    "def revid_from_date(article, language, date=False):\n",
    "    '''\n",
    "    Get the revision ID of an article at a particular point in time\n",
    "    '''\n",
    "    params = {'titles':article, 'prop':'revisions', 'rvstart':date, 'rvprop':'ids|timestamp', 'rvlimit':1}\n",
    "    if not date:\n",
    "        params['rvdir'] = 'older'\n",
    "        del params['rvstart']\n",
    "    try:\n",
    "        return list(next(query(params, language))['pages'].values())[0]['revisions'][0]['revid']\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network generators (click to expand):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikilink_network(articles, language, fix_outer_redirects = False):\n",
    "    '''\n",
    "    Create a network of Wikipedia articles.\n",
    "    1. Take a list of core articles, get all of the out-links from and in-links to these articles.\n",
    "    2. Get all the links between articles in the larger set of pages.\n",
    "    (Optional 3. Resolve redirects on the 1.5 hop links, this is time consuming)\n",
    "    \n",
    "    This function can take a long time for articles with many links, or articles that are very popular across Wikipedia e.g. 'United States'\n",
    "    '''\n",
    "    \n",
    "    \n",
    "#    fix redirects on initial set\n",
    "\n",
    "#    get neighbours\n",
    "    print('Getting outlinks')\n",
    "    page_chunks = ['|'.join(x) for x in chunks(articles, 50)]\n",
    "    outlinks = []\n",
    "    for pages in page_chunks:\n",
    "        out_params = {'titles':pages, 'prop':'links', 'pllimit':'max', 'plnamespace':'0'}\n",
    "        outlinks.extend([(v['title'], x['title']) for p in query(out_params, language) for v in p['pages'].values() for x in v.get('links', [])])\n",
    "\n",
    "    print('Getting inlinks')        \n",
    "    inlinks = []\n",
    "    for page in articles:\n",
    "        in_params = {'list':'backlinks', 'bltitle':page, 'blnamespace':'0', 'bllimit':'max', 'blfilterredir':'nonredirects', 'blredirect':'True'} # redirects x 2\n",
    "        for i in query(in_params, 'en'):            \n",
    "            inlinks.extend([(x['title'], page) for x in i['backlinks'] if 'redirect' not in x.keys()])\n",
    "            inlinks.extend([(y['title'], page) for x in i['backlinks'] for y in x.get('redirlinks', [])])\n",
    "\n",
    "    redirects = {}\n",
    "    all_articles = list((set([x[1] for x in outlinks])|set([x[0] for x in inlinks])) - set(articles))\n",
    "    print('Getting links between %d pages' %len(all_articles))\n",
    "    page_chunks = ['|'.join(x) for x in chunks(all_articles, 50)]\n",
    "    links = []\n",
    "    for n, pages in enumerate(page_chunks):\n",
    "        print('%.2f%%' %(100*n/len(page_chunks)))\n",
    "        out_params = {'titles':pages, 'prop':'links', 'pllimit':'max', 'plnamespace':'0'}\n",
    "        for p in query(out_params, language):\n",
    "            if 'redirects' in p.keys():\n",
    "                redirects.update({x['from']:x['to'] for x in p['redirects']})\n",
    "            links.extend([(v['title'], x['title']) for v in p['pages'].values() for x in v.get('links', [])])\n",
    "    \n",
    "    if fix_outer_redirects:\n",
    "        print('Fixing outer redirects over %d pages' %len(set([i[1] for i in links])))\n",
    "        page_chunks = ['|'.join(x) for x in chunks(list(set([i[1] for i in links])), 50)]\n",
    "        for n, pages in enumerate(page_chunks):\n",
    "            if n%50==0: print('%.2f%%' %(100*n/len(page_chunks)))\n",
    "            rd_params = {'titles':pages}\n",
    "            redirects.update({x['from']:x['to'] for x in next(query(rd_params, language)).get('redirects', [])})\n",
    "\n",
    "    all_articles = [redirects.get(x, x) for x in all_articles]\n",
    "    outlinks = [(x[0], redirects.get(x[1], x[1])) for x in outlinks]\n",
    "    links = [(x[0], redirects.get(x[1], x[1])) for x in links if redirects.get(x[1], x[1]) in all_articles+articles]\n",
    "    \n",
    "    edgelist = list(set(outlinks + inlinks + links))\n",
    "    \n",
    "#    fix redirects on new set\n",
    "    \n",
    "    return nx.DiGraph(edgelist)\n",
    "  \n",
    "       \n",
    "def content_network(articles, language, sectionids=None, date = None):\n",
    "    '''\n",
    "    Create a network of Wikipedia articles based on links in page content\n",
    "    This can be the whole page (akin to wikilink_network), but is better suited to picking links from an particular sections,\n",
    "    such as the introduction.\n",
    "    Section ids is a list of the sections requested, sections start at id=0.\n",
    "    \n",
    "    This function can also generate networks from a particular point in time.\n",
    "    \n",
    "    Note that we only generate the network from outlinks, not inlinks.\n",
    "    '''\n",
    "#    global params\n",
    "\n",
    "    print('getting core links')\n",
    "    links = []\n",
    "    redirects = {}\n",
    "    for i in articles:\n",
    "        if date:\n",
    "            revid = revid_from_date(i, language, date)\n",
    "            if revid:\n",
    "                params = {'oldid':revid, 'prop':'links'}\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            params = {'page':i, 'prop':'links'}\n",
    "        if sectionids:\n",
    "            for sectionid in sectionids:\n",
    "                params['section'] = sectionid\n",
    "                for p in parse(params, language):\n",
    "                    links.extend([(p['title'], x['*']) for x in p['links'] if x['ns']==0])\n",
    "        else:\n",
    "            for p in parse(params, language):\n",
    "                links.extend([(p['title'], x['*']) for x in p['links'] if x['ns']==0])\n",
    "    \n",
    "    redirects = {}\n",
    "    newpages = set([x[1] for x in links])-set(articles)     \n",
    "    print('getting links from %d articles' %len(newpages))\n",
    "    for n, i in enumerate(newpages):\n",
    "        if n%20==0: print('%.2f%%' %(100*n/len(newpages)))\n",
    "        if date:\n",
    "            revid = revid_from_date(i, language, date)\n",
    "            if revid:\n",
    "                params = {'oldid':revid, 'prop':'links'}\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            params = {'page':i, 'prop':'links'}\n",
    "        if sectionids:\n",
    "            for sectionid in sectionids:\n",
    "                params['section'] = sectionid \n",
    "                for p in parse(params, language):\n",
    "                    redirects[i] = p['title']\n",
    "                    links.extend([(p['title'], x['*']) for x in p['links'] if x['ns']==0])\n",
    "        else:\n",
    "            for p in parse(params, language):\n",
    "                redirects[i] = p['title']\n",
    "                links.extend([(p['title'], x['*']) for x in p['links'] if x['ns']==0])\n",
    "   \n",
    "    newpages = [redirects.get(x, x) for x in newpages]\n",
    "    links = [(x[0], redirects.get(x[1], x[1])) for x in links]\n",
    "    edgelist = pd.DataFrame(list(set([x for x in links if x[1] in list(newpages)+articles])))\n",
    "    edgelist.columns = ['source', 'target']\n",
    "    \n",
    "    return nx.from_pandas_edgelist(edgelist)\n",
    "\n",
    "\n",
    "\n",
    "def clickstream_network(articles, clickstream_df, cutoff = 0, depth = 1, get_redirects = True):\n",
    "    '''\n",
    "    Generate weighted network based on clickstream data (with an optional cutoff for number of link transitions).\n",
    "    1. Take a list of core articles, get all of the out-links from and in-links to these articles.\n",
    "    2. Get all the links between articles in the larger set of pages.\n",
    "    3. Repeat up to a specified depth\n",
    "    \n",
    "    Much quicker than wikilink_network, content_network.\n",
    "    '''\n",
    "    \n",
    "# get redirects of initial set\n",
    "#     get network like other fn\n",
    "    articles_u = [x.replace(' ', '_') for x in articles]\n",
    "    dfl = clickstream_df.copy()\n",
    "    if cutoff > 0 :\n",
    "        dfl = dfl[dfl['n']>cutoff]\n",
    "        \n",
    "    all_articles = []\n",
    "    for i in range(depth):\n",
    "        in_n = dfl[dfl['curr'].isin(articles_u) == True]['prev']\n",
    "        out_n = dfl[dfl['prev'].isin(articles_u) == True]['curr']\n",
    "        all_articles = list(set(articles_u) | set(in_n) | set(out_n))        \n",
    "        articles_u = all_articles\n",
    "#        dfredict[start].append(articles)\n",
    "            \n",
    "    print('getting links')\n",
    "    edgelist = dfl[(dfl['curr'].isin(all_articles) == True)&(dfl['prev'].isin(all_articles) == True)][['prev', 'curr', 'n']]\n",
    "    edgelist.columns = ['source', 'target', 'weight']\n",
    "    edgelist['source'] = edgelist['source'].str.replace('_', ' ')\n",
    "    edgelist['target'] = edgelist['target'].str.replace('_', ' ')\n",
    "    \n",
    "    return nx.from_pandas_edgelist(edgelist, edge_attr='weight')\n",
    "\n",
    "\n",
    "def multi_lang_networks(articles, orig_language, languages, network_type, **kwargs):\n",
    "    '''\n",
    "    Take list of articles in one language, generate Wiki network in several language.\n",
    "    1. Gets interlanguage links of supplied list of articles\n",
    "    2. Generate network in each language according to specified method, return as dictionary\n",
    "    3. Supplies dictionary of translations for each new network\n",
    "    '''\n",
    "    lang_dict = {l:{} for l in languages}\n",
    "    network_dict = {}\n",
    "    translations = {l:{} for l in languages}\n",
    "    page_chunks = ['|'.join(x) for x in chunks(articles, 50)]\n",
    "    \n",
    "    print(orig_language)\n",
    "    kwargs2 = kwargs.copy()\n",
    "    if network_type == wikilink_network or network_type == content_network:\n",
    "        kwargs2['language'] = orig_language        \n",
    "    if network_type == clickstream_network:\n",
    "        kwargs2['clickstream_df'] = kwargs['clickstream_dfs'][orig_language]\n",
    "        del kwargs2['clickstream_dfs']\n",
    "    network_dict[orig_language] = network_type(articles, **kwargs2)\n",
    "    translations[orig_language] = {x:x for x in network_dict[orig_language].nodes()}\n",
    "    \n",
    "    for l in languages:\n",
    "        print(l)\n",
    "        for i in page_chunks:\n",
    "            params = {'titles':i, 'prop':'langlinks', 'lllang':l}\n",
    "            lang_dict[l].update({y['title']:y['langlinks'][0]['*']  for x in query(params, orig_language)\n",
    "                for y in x['pages'].values() if 'langlinks' in y.keys()})\n",
    "        if set(articles)-set(lang_dict[l].keys()):\n",
    "            print('Some articles not included in %s network' %l, set(articles)-set(lang_dict[l].keys()))\n",
    "            \n",
    "        kwargs2 = kwargs.copy()\n",
    "        if network_type == wikilink_network or network_type == content_network:\n",
    "            kwargs2['language'] = l        \n",
    "        if network_type == clickstream_network:\n",
    "            kwargs2['clickstream_df'] = kwargs['clickstream_dfs'][l]\n",
    "            del kwargs2['clickstream_dfs']\n",
    "            \n",
    "        network_dict[l] = network_type(list(lang_dict[l].values()), **kwargs2)\n",
    "        \n",
    "        print('getting translations')\n",
    "        trans_chunks = ['|'.join(x) for x in chunks(list(network_dict[l].nodes()), 50)]\n",
    "        for i in trans_chunks:\n",
    "            params = {'titles':i, 'prop':'langlinks', 'lllang':orig_language}\n",
    "            translations[l].update({y['title']:y['langlinks'][0]['*']  for x in query(params, l)\n",
    "                    for y in x['pages'].values() if 'langlinks' in y.keys()})\n",
    "       \n",
    "    return network_dict, translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network attribute functions (click to expand):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_properties(network, wikiquery, language, fix_redirects = True):\n",
    "    '''\n",
    "    A general function to attach properties from wiki queries to nodes in a network.\n",
    "    Doesn't handle all properties appropriately, some data cleaning likely required afterwards.\n",
    "    '''\n",
    "    \n",
    "    page_chunks = ['|'.join(x) for x in chunks(list(network.nodes()), 1)]\n",
    "    p_dict  = {p:[] for p in list(network.nodes())}\n",
    "\n",
    "    for n, pages in enumerate(page_chunks):\n",
    "        print(n/len(page_chunks))\n",
    "        params = {'titles':pages, **wikiquery}\n",
    "        for p in query(params, language):\n",
    "            for v in p['pages'].values():\n",
    "                p_dict[v['title']].extend([x for x in v.get(params['prop'], [])])\n",
    "    \n",
    "    if all([len(x) <=1 for x in p_dict.values()]):\n",
    "        p_dict = {k:next(iter(v), None) for k,v in p_dict.items()}\n",
    "    \n",
    "    for i in network.nodes():\n",
    "        network.nodes[i][params['prop']] = p_dict[i]\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "def nodes_to_dataframe(network, properties = None):\n",
    "    '''\n",
    "    Outputs nodes & their associated attributes to a DataFrame\n",
    "    '''\n",
    "    output = pd.DataFrame(dict(network.nodes(data=True))).T    \n",
    "    if properties:\n",
    "        output = output[properties]\n",
    "    \n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Networks <a name=\"generatingnetworks\"></a>\n",
    "### Wikilinks network <a name=\"wikilinks\"></a>\n",
    "First off, let's create a network of articles based around the core articles 'Emmanuel Macron' and 'Angela Merkel'.\n",
    "We'll use the 'wikilink_network' function, which gets all links to and from the supplied core articles, and all links between their neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['Emmanuel Macron', 'Angela Merkel']\n",
    "\n",
    "language = 'en'\n",
    "\n",
    "G1 = wikilink_network(articles, language) # Generate network\n",
    "print(nx.info(G1)) # print info\n",
    "nx.write_graphml(G1, 'WikiNetwork.graphml') # save graph for import into Gephi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Whilst this network is 'complete' there are a number of problems;\n",
    "\n",
    "1. Quite slow to generate so many links.\n",
    "2. All links are ranked as equally relevant, regardless of where they are or how frequently they are clicked on in the page.\n",
    "   (We often get spurious links e.g. to the 'International Standard Book Number' page)\n",
    "3. Limited to the most recent versions of the pages.\n",
    "\n",
    "### Content-based network <a name=\"content\"></a>\n",
    "\n",
    "One way of getting around these issues is to generate a network based on the content of a page (or a revision), and to select hyperlinks based on where they appear, typically the introduction. For this we use the 'content_network' function. We can also use this function to generate an article network from a particular point in time. Very useful for tracking how the information structure changes through time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = ['Emmanuel Macron', 'Angela Merkel']\n",
    "\n",
    "G2 = content_network(articles, language, sectionids=[0]) # Generate network\n",
    "G2a = content_network(articles, language, sectionids=[0], date = '20150101T000000') # Generate network as it was on 1st Jan 2015\n",
    "\n",
    "print(nx.info(G2)) # print info\n",
    "print(nx.info(G2a))\n",
    "\n",
    "nx.write_graphml(G2, 'ContentNetwork.graphml') # save graphs for import into Gephi\n",
    "nx.write_graphml(G2a, 'ContentNetwork2015.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G2) # specify node positions\n",
    "nx.draw_networkx_nodes(G2, pos, node_size=20) # draw nodes\n",
    "nx.draw_networkx_edges(G2, pos) # draw edges\n",
    "plt.show()\n",
    "\n",
    "pos2 = nx.spring_layout(G2a, pos=pos) # specify node positions\n",
    "pos2.update(pos)\n",
    "nx.draw_networkx_nodes(G2a, pos2, node_size=20) # draw nodes\n",
    "nx.draw_networkx_edges(G2a, pos2) # draw edges\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goes some of the way towards addressing our previous concerns. However;\n",
    "1. Selecting links by section is an imperfect heuristic for choosing 'relevant' links.\n",
    "2. Links are unweighted, so we have no sense of _how_ relevant pages are to each other.\n",
    "\n",
    "### Clickstream Network <a name=\"clickstream\"></a>\n",
    "\n",
    "We can turn to the Wikipedia Clickstream dataset; a list of all the links between pages on Wikipedia, and how often they are clicked on.\n",
    "\n",
    "This gives us links on a page that are selected for by relevance, as determined by how frequently they are used.\n",
    "\n",
    "An added perk is that since this data is stored locally, the whole procedure is much quicker. We can even quickly generate 2-hop, 3-hop networks. I highly recommend using this method on the larger Wikipedias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G3 = clickstream_network(articles, clickstream_dfs['en']) # generate clickstream network\n",
    "G3a = clickstream_network(articles, clickstream_dfs['en'], cutoff=500, depth=2) # generate clickstream network, with a minimum edge weight of 1000 and a search depth of2\n",
    "\n",
    "print(nx.info(G3)) # print info\n",
    "print(nx.info(G3a))\n",
    "\n",
    "nx.write_graphml(G3, 'ClickstreamNetwork.graphml') # save graphs\n",
    "nx.write_graphml(G3a, 'ClickstreamNetworkc1000d2.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G3)\n",
    "nx.draw_networkx_nodes(G3, pos, node_size=20)\n",
    "nx.draw_networkx_edges(G3, pos)\n",
    "plt.show()\n",
    "\n",
    "pos2 = nx.spring_layout(G3a, pos=pos)\n",
    "pos2.update(pos)\n",
    "nx.draw_networkx_nodes(G3a, pos2, node_size=20)\n",
    "nx.draw_networkx_edges(G3a, pos2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying a Network <a name=\"studying\"></a>\n",
    "Now that we are able to generate a network of articles, let's explore what we can do with them. We're going to:\n",
    "\n",
    "1. Generate clickstream networks in multiple languages from the same core articles.\n",
    "2. Get data on identity of editors and size of edits\n",
    "3. Compute overlap of editors between languages (Finding multilingual editors)\n",
    "4. Does article centrality correlate between languages? Are the important articles the same in different languages?\n",
    "5. Compare network centrality against no. of editors & size of edits. Do the most visited articles receive more edits, editors and/or larger edits?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate Clickstream networks\n",
    "orig_language = 'en'\n",
    "articles = ['Emmanuel Macron', 'Angela Merkel']\n",
    "languages = ['fr', 'de']\n",
    "\n",
    "networks, translations = multi_lang_networks(articles, orig_language, languages, clickstream_network, **{'clickstream_dfs':clickstream_dfs, 'cutoff':0})\n",
    "\n",
    "for k, i in networks.items(): # print network info\n",
    "    print(k, nx.info(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Get data on revisions, editors, and edit size for each network (in 2019)\n",
    "for k, v in networks.items():\n",
    "    print(k)\n",
    "    get_network_properties(v, {'prop':'revisions', 'rvprop':'user|size', 'rvlimit':'max', 'rvend':'20190101T000000'}, k, fix_redirects = True)\n",
    "    nx.set_node_attributes(v, translations[k], name='translation')\n",
    "\n",
    "nodes_df = {k: nodes_to_dataframe(v) for k, v in networks.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compile lists of editors in each language and print the overlap between language editions\n",
    "editors = {k:set([x.get('user', None) for y in v['revisions'] for x in y ]) for k, v in nodes_df.items()}\n",
    "\n",
    "for i in langlist:\n",
    "    for j in langlist:\n",
    "        if i != j:\n",
    "            perc = 100*len(editors[i] & editors[j])/len(editors[i])\n",
    "            print('%.1f%% of %s.wiki editors also edit %s.wiki' %(perc, i, j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate scatter plots for centrality of articles present in 2 Wiki languages\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "for k, v in networks.items():\n",
    "    nodes_df[k]['Centrality'] = pd.Series(nx.pagerank(v))\n",
    "\n",
    "for n, i in enumerate(langlist):\n",
    "    for m, j in enumerate(langlist):\n",
    "        if m>n:\n",
    "            overlap = set(nodes_df[i]['translation'])&set(nodes_df[j]['translation'])-set([np.nan])\n",
    "            x = nodes_df[i].set_index('translation')[nodes_df[i].set_index('translation').index.isin(overlap)]['Centrality']\n",
    "            y = nodes_df[j].set_index('translation')[nodes_df[j].set_index('translation').index.isin(overlap)]['Centrality']\n",
    "            print(len(x), len(y), len(overlap))\n",
    "            plt.scatter(np.log10(x), np.log10(y), alpha=0.4)\n",
    "            plt.ylabel(j+' centrality')\n",
    "            plt.xlabel(i+' centrality')\n",
    "            print(pearsonr(x,y))\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Generate scatter plots for centrality vs. number of editors, number of revisions & size of revisions.\n",
    "\n",
    "for k, v in networks.items():\n",
    "    nodes_df[k]['revisionsize'] = nodes_df[k]['revisions'].apply(lambda x: np.median([z['size'] for z in x]))\n",
    "    nodes_df[k]['n_revisions'] = nodes_df[k]['revisions'].apply(lambda x: len(x))\n",
    "    nodes_df[k]['n_editors'] = nodes_df[k]['revisions'].apply(lambda x: len(set([z.get('user', None) for z in x])))\n",
    "\n",
    "    data = nodes_df[k][['Centrality', 'revisionsize', 'n_revisions', 'n_editors']].dropna(subset=['revisionsize'])\n",
    "    for n,i in enumerate(['n_revisions', 'n_editors', 'revisionsize']):\n",
    "        plt.scatter(np.log10(data['Centrality']), np.log10(data[i]), alpha=0.4)\n",
    "        plt.ylabel('log10(%s)' %i)\n",
    "        plt.xlabel('log10(Centrality)')\n",
    "        plt.title('%s network' %k)\n",
    "        print(pearsonr(np.log10(data['Centrality']), np.log10(data[i])))\n",
    "        plt.show()    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Running a Query <a name=\"query\"></a>\n",
    "\n",
    "This skill has been skimmed over in this tutorial, and much of the heavy lifting has been handled in the pre-made functions. But if you want to run your own queries to download data on Wikipedia pages the following might be helpful.\n",
    "\n",
    "Documentation available here: https://www.mediawiki.org/wiki/API:Properties\n",
    "\n",
    "We can run a few queries to see the type of data being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = 'London|Berlin|Paris'\n",
    "\n",
    "params = {'titles':articles, 'prop':'revisions'}\n",
    "print(list(query(params, 'en')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'titles':articles, 'prop':'links', 'pllimit':'max'}\n",
    "for i in query(params, 'en'):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
